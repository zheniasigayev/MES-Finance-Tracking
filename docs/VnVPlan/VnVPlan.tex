\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}

\wss{Say what software is being tested.  Give its name and a brief overview of
  its general functions.}

The MES Finance Tracking Platform is a web application that centralizes club reimbursement submissions, 
reviews, and approvals for the MES with role based access and receipt storage. The front end user interface 
will be tested for usability and functionality to ensure a smooth user experience. The back end data storage
will be tested for data integrity and security to ensure sensitive financial information is protected. The database
will be tested for performance and reliability to ensure quick access to financial records. 

\subsection{Objectives}

\wss{State what is intended to be accomplished.  The objective will be around
  the qualities that are most important for your project.  You might have
  something like: ``build confidence in the software correctness,''
  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
  just those that are most important.}

\wss{You should also list the objectives that are out of scope.  You don't have 
the resources to do everything, so what will you be leaving out.  For instance, 
if you are not going to verify the quality of usability, state this.  It is also 
worthwhile to justify why the objectives are left out.}

\wss{The objectives are important because they highlight that you are aware of 
limitations in your resources for verification and validation.  You can't do everything, 
so what are you going to prioritize?  As an example, if your system depends on an 
external library, you can explicitly state that you will assume that external library 
has already been verified by its implementation team.}

\noindent In Scope Objectives:
\begin{itemize}
  \item Demonstrate software correctness of end-to-end user interface reimbursement workflow
  \item Ensure data integrity and security of sensitive financial information 
  \item Meet all performance/capacity targets for database access times
  \item Ensure financial accuracy of reimbursement camera captures and calculations
\end{itemize}

\noindent Out of Scope Objectives:
\begin{itemize}
  \item Comprehensive usability testing for all potential user personas due to limited resources.
\end{itemize}


\subsection{Extras}

\wss{Summarize the extras (if any) that were tackled by this project.  Extras
can include usability testing, code walkthroughs, user documentation, formal
proof, GenderMag personas, Design Thinking, etc.  Extras should have already
been approved by the course instructor as included in your problem statement.
You can use a pull request to update your extras (in TeamComposition.csv or
Repos.csv) if your plan changes as a result of the VnV planning exercise.}

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (design documents, like MG, MIS, etc).  You
  can include these even before they are written, since by the time the project
  is done, they will be written.  You can create BibTeX entries for your
  documents and within those entries include a hyperlink to the documents.}

\citet{SRS}

\wss{Don't just list the other documents.  You should explain why they are relevant and 
how they relate to your VnV efforts.}

\section{Plan}

\wss{Introduce this section.  You can provide a roadmap of the sections to
  come.}

\subsection{Verification and Validation Team}

\wss{Your teammates.  Maybe your supervisor.
  You should do more than list names.  You should say what each person's role is
  for the project's verification.  A table is a good way to summarize this information.}

\subsection{SRS Verification}

\wss{List any approaches you intend to use for SRS verification.  This may
  include ad hoc feedback from reviewers, like your classmates (like your
  primary reviewer), or you may plan for something more rigorous/systematic.}

\wss{If you have a supervisor for the project, you shouldn't just say they will
read over the SRS.  You should explain your structured approach to the review.
Will you have a meeting?  What will you present?  What questions will you ask?
Will you give them instructions for a task-based inspection?  Will you use your
issue tracker?}

\wss{Maybe create an SRS checklist?}

\subsection{Design Verification}

\wss{Plans for design verification}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification}

\wss{The verification and validation plan is an artifact that should also be
verified.  Techniques for this include review and mutation testing.}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Implementation Verification}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static
  verification of the implementation.  Potential techniques include code
  walkthroughs, code inspection, static analyzers, etc.}

\wss{The final class presentation in CAS 741 could be used as a code
walkthrough.  There is also a possibility of using the final presentation (in
CAS741) for a partial usability survey.}

\subsection{Automated Testing and Verification Tools}

\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}

\wss{If you have already done this in the development plan, you can point to
that document.}

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}

\subsection{Software Validation}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\wss{You might want to use review sessions with the stakeholder to check that
the requirements document captures the right requirements.  Maybe task based
inspection?}

\wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
be used as an opportunity to validate the requirements.  You should plan on 
demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
The feedback from your supervisor will be very useful for improving your project.}

\wss{For teams without an external supervisor, user testing can serve the same purpose 
as a Rev 0 demo for the supervisor.}

\wss{This section might reference back to the SRS verification section.}

\section{System Tests}

\wss{There should be text between all headings, even if it is just a roadmap of
the contents of the subsections.}

This section describes the planned system tests for verifying the MES Finance Tracking Platform. 
Each subsection corresponds to one or more functional requirements defined in the SRS.
 The goal is to ensure that all reimbursement-related workflows—submission, review, and status tracking—perform as intended.

\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.}

The following tests verify that the MES Finance Tracking Platform correctly implements the functional requirements defined in Section 9.1 of the SRS. 
These tests ensure that all features related to reimbursement submission, review, and audit logging behave as expected.

\subsubsection{Area of Testing — Reimbursement Submission}

This area covers \textbf{Functional Requirements FRQ-1} and \textbf{FRQ-4}, ensuring that club members can submit expense claims with attached digital receipts and that data is permanently stored.

\textbf{Title for Test:} Submit Expense Reimbursement Request \\
\textbf{Test-id:} FRQ-1-SUBMIT

\begin{itemize}
    \item \textbf{Control:} Manual versus Automatic \\
    Automatic (performed using UI automation with manual verification)
    \item \textbf{Initial State:} \\
    The user is logged in as a valid club member with an existing club ID. No reimbursement requests are currently active for that user.
    \item \textbf{Input:} \\
    Receipt file (PDF or image $\leq$10 MB), amount = \$100, reimbursement form fields completed.
    \item \textbf{Output (Expected Result):} \\
    The request appears in the user’s dashboard with status = ``Submitted.'' The uploaded receipt is stored and accessible under that request.
    \item \textbf{Test Case Derivation:} \\
    Based on BUC-2 (``Submitting Reimbursement Request'') and FRQ-1/FRQ-4 in the SRS. Confirms form submission, file upload, and database persistence.
    \item \textbf{How Test Will Be Performed:} \\
    Using Cypress or Playwright to automate UI submission, then verifying MongoDB entries and front-end confirmation.
\end{itemize}

\subsubsection{Area of Testing — Reimbursement Review Workflow}

This area verifies \textbf{FRQ-2}, \textbf{FRQ-3}, and \textbf{FRQ-5}, ensuring that MES reviewers can view submissions, approve/reject them, and that actions are recorded in the audit trail.

\textbf{Title for Test:} Review and Approve Reimbursement Request \\
\textbf{Test-id:} FRQ-2-REVIEW

\begin{itemize}
    \item \textbf{Control:} Manual versus Automatic \\
    Semi-automatic (UI actions by MES reviewer, back-end verification automated)
    \item \textbf{Initial State:} \\
    A reimbursement request exists in ``Submitted'' status. MES reviewer is logged in with admin privileges.
    \item \textbf{Input:} \\
    Reviewer selects request $\rightarrow$ clicks ``Approve'' $\rightarrow$ enters approval note.
    \item \textbf{Output (Expected Result):} \\
    Request status = ``Approved.'' Submitter is notified. Audit log records: \{timestamp, reviewerID, action: Approved\}.
    \item \textbf{Test Case Derivation:} \\
    Based on BUC-3 (``Reviewing Reimbursement Request'') and FRQ-2/FRQ-5 in the SRS.
    \item \textbf{How Test Will Be Performed:} \\
    UI interaction test plus backend log verification using database queries to ensure audit trail persistence.
\end{itemize}

\subsubsection{Area of Testing — Reimbursement Status Tracking}

This test verifies \textbf{FRQ-3} and \textbf{FRQ-6}, ensuring that submitters and authorized users can view real-time reimbursement statuses.

\textbf{Title for Test:} View Reimbursement Status \\
\textbf{Test-id:} FRQ-3-STATUS

\begin{itemize}
    \item \textbf{Control:} Manual versus Automatic \\
    Automatic (API and UI check)
    \item \textbf{Initial State:} \\
    Multiple reimbursement requests exist with different statuses (submitted, under review, approved).
    \item \textbf{Input:} \\
    User navigates to the ``My Requests'' page or queries the API endpoint \texttt{/api/reimbursements}.
    \item \textbf{Output (Expected Result):} \\
    The UI displays the correct status for each request. The API returns accurate data matching the database state.
    \item \textbf{Test Case Derivation:} \\
    Derived from FRQ-3 and FRQ-6, validating that access is role-based and statuses are synchronized with the database.
    \item \textbf{How Test Will Be Performed:} \\
    Automated API tests (Postman/pytest) and front-end UI verification to cross-validate consistency.
\end{itemize}

\subsubsection{Area of Testing — Receipt Persistence and Security}

This area validates the data integrity requirement from \textbf{FRQ-4} and \textbf{Security Requirements INT-1, PRI-1}.

\textbf{Title for Test:} Verify Receipt Storage Security \\
\textbf{Test-id:} FRQ-4-STORAGE

\begin{itemize}
    \item \textbf{Control:} Automatic
    \item \textbf{Initial State:} \\
    One reimbursement submission with a receipt attached.
    \item \textbf{Input:} \\
    Query the database and storage bucket for the receipt file.
    \item \textbf{Output (Expected Result):} \\
    Receipt file is encrypted at rest (per SRS Section 16.3), accessible only to authorized users, and retrievable without corruption.
    \item \textbf{Test Case Derivation:} \\
    From FRQ-4 and SRS Section 16 (INT, PRI).
    \item \textbf{How Test Will Be Performed:} \\
    Check encryption metadata and permissions via the cloud storage API (AWS S3 or Vercel storage).
\end{itemize}

\subsubsection{Area of Testing — Audit Logging}

This area ensures \textbf{FRQ-5} is satisfied by verifying audit log entries for all major actions.

\textbf{Title for Test:} Audit Log Verification \\
\textbf{Test-id:} FRQ-5-AUDIT

\begin{itemize}
    \item \textbf{Control:} Automatic
    \item \textbf{Initial State:} \\
    System contains multiple reimbursement records with various user actions.
    \item \textbf{Input:} \\
    Run log export query for ``Approve'' or ``Reject'' events.
    \item \textbf{Output (Expected Result):} \\
    Each event entry includes timestamp, userID, actionType, and status change. Logs persist for at least 3 years per SRS Section 16.4.
    \item \textbf{Test Case Derivation:} \\
    Derived from FRQ-5 and Audit Requirements [AUD-1, AUD-2].
    \item \textbf{How Test Will Be Performed:} \\
    Automated test script queries audit collection and validates schema and retention policies.
\end{itemize}


\subsection{Tests for Nonfunctional Requirements}

The following tests verify that the MES Finance Tracking Platform meets the nonfunctional requirements defined in the SRS. These include performance, usability, security, and maintainability tests to ensure the system is robust, efficient, and user-friendly.

\subsubsection{Area of Testing — Performance and Latency}

This area covers \textbf{Speed and Latency Requirements [SaL]} and partially overlaps with \textbf{Robustness or Fault-Tolerance Requirements [FLT]}.

\textbf{Title for Test:} API and Page Load Performance Test \\
\textbf{Test-id:} NFL-SAL-1

\begin{itemize}
    \item \textbf{Control:} Automatic
    \item \textbf{Initial State:} \\
    System deployed to staging environment with seeded database (100 clubs, 1,000 reimbursement requests).
    \item \textbf{Input:} \\
    Execute 100 concurrent API requests for reimbursement data and record response times. Load homepage and dashboard in Chrome using Lighthouse.
    \item \textbf{Output (Expected Result):} \\
    API response times average below 1 second; page load times under 3 seconds; file uploads ($<10$ MB) complete within 5 seconds.
    \item \textbf{Test Case Derivation:} \\
    Derived from SRS Section 13.1 [SaL-1, SaL-2, SaL-3].
    \item \textbf{How Test Will Be Performed:} \\
    Automated load testing with tools such as JMeter or Locust, combined with Lighthouse performance scoring. Results summarized in a table comparing observed times to SRS thresholds.
\end{itemize}

\subsubsection{Area of Testing — Usability and Accessibility}

This area validates \textbf{Ease of Use and Learning [EUL]}, \textbf{Accessibility [ABL]}, and \textbf{Style [STY]} requirements.

\textbf{Title for Test:} Usability and Accessibility Evaluation \\
\textbf{Test-id:} NFL-USAB-1

\begin{itemize}
    \item \textbf{Control:} Manual (survey-based)
    \item \textbf{Initial State:} \\
    A functioning system prototype accessible by 5 test users representing different roles (club member, executive, MES reviewer).
    \item \textbf{Input:} \\
    Users perform typical tasks (submit reimbursement, review request, check status). Afterward, they complete a usability survey using a 5-point Likert scale.
    \item \textbf{Output (Expected Result):} \\
    Average ease-of-use rating $\geq$ 4/5. All UI components are keyboard-navigable and meet WCAG 2.1 AA color contrast.
    \item \textbf{Test Case Derivation:} \\
    Based on SRS Sections 11 and 12, especially [EUL-1] and [ABL-1 – ABL-5].
    \item \textbf{How Test Will Be Performed:} \\
    Observation-based usability session followed by survey analysis. Accessibility verified with automated tools (axe-core, Lighthouse accessibility audits).
\end{itemize}

\subsubsection{Area of Testing — Security and Privacy}

This area ensures compliance with \textbf{Access [ACS]}, \textbf{Integrity [INT]}, and \textbf{Privacy [PRI]} requirements.

\textbf{Title for Test:} Role-Based Access and Data Protection Test \\
\textbf{Test-id:} NFL-SEC-1

\begin{itemize}
    \item \textbf{Control:} Automatic with manual verification
    \item \textbf{Initial State:} \\
    Test users include a general member, club executive, and MES admin, each authenticated with proper roles.
    \item \textbf{Input:} \\
    Attempt unauthorized access to restricted endpoints (e.g., reviewing reimbursements as a non-admin). Submit tampered form payloads.
    \item \textbf{Output (Expected Result):} \\
    Unauthorized requests receive HTTP 403 responses; input validation prevents injection/XSS; all communication uses HTTPS (TLS 1.2+). Sensitive fields are encrypted in the database.
    \item \textbf{Test Case Derivation:} \\
    Derived from SRS Section 16: [ACS-1, INT-1, PRI-1].
    \item \textbf{How Test Will Be Performed:} \\
    Automated penetration testing with OWASP ZAP or Burp Suite; verification of encryption and RBAC through API inspection and code review.
\end{itemize}

\subsubsection{Area of Testing — Maintainability and Supportability}

This area validates \textbf{Maintenance [MNT]} and \textbf{Supportability [SUP]} requirements.

\textbf{Title for Test:} Code Quality and Documentation Compliance \\
\textbf{Test-id:} NFL-MNT-1

\begin{itemize}
    \item \textbf{Control:} Static test (manual review and CI enforcement)
    \item \textbf{Initial State:} \\
    Project repository set up with ESLint and Prettier configuration as specified in the SRS.
    \item \textbf{Input:} \\
    Run code through linting and formatting checks; review developer documentation for completeness.
    \item \textbf{Output (Expected Result):} \\
    No critical linting errors; documentation covers setup, deployment, and troubleshooting. All dependencies are updated to current minor versions.
    \item \textbf{Test Case Derivation:} \\
    Derived from SRS Section 15 [MNT-1, SUP-1, SUP-2].
    \item \textbf{How Test Will Be Performed:} \\
    CI pipeline enforces linting and formatting. Manual inspection ensures documentation matches environment setup and maintenance requirements.
\end{itemize}


\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

  This table maps the high-level Functional Requirements to the supporting Non-Functional Requirements that must be considered during testing to ensure the system's overall quality, security, and performance.

\vspace{0.5cm}

\begin{longtable}{p{0.4\linewidth} p{0.55\linewidth}}
\caption{Traceability Matrix for MES Finance Tracking Platform} \\
\toprule
\textbf{Functional Requirement (FRQ)} & \textbf{Supporting Non-Functional Requirement IDs (NFR)} \\
\midrule
\endfirsthead
\toprule
\textbf{Functional Requirement (FRQ)} & \textbf{Supporting Non-Functional Requirement IDs (NFR)} \\
\midrule
\endhead
\bottomrule
\endfoot
\textbf{FRQ-1:} The system shall allow MES clubs and teams to submit expense claims. &
EUL-1, UaP-2, ABL-1, ABL-3, ABL-5, SaL-1, SaL-2, FLT-2, CAP-1, ACS-1, ACS-2, INT-1, PRI-1, PRI-2, AUD-1, CUL-1, SEI-2, AUL-1, STD-1, STD-2 \\
\midrule
\textbf{FRQ-2:} The system shall provide MES reviewers with tools to efficiently review, approve, or reject the reimbursement requests. &
APP-1, APP-2, STY-3, EUL-1, ABL-1, ABL-5, SaL-1, PRE-1, FLT-1, CAP-1, ACS-1, ACS-2, INT-1, PRI-1, PRI-2, AUD-1, AUD-2, CUL-1, SEI-2, AUL-1, AUL-2, STD-1, STD-2 \\
\midrule
\textbf{FRQ-3:} The system shall track the status of each expense claim (e.g., submitted, under review, approved, rejected, reimbursed). &
APP-2, STY-3, ABL-1, SaL-1, SaL-2, FLT-3, CAP-2, EXT-2, LNG-2, ACS-1, ACS-2, PRI-1, AUD-1, AUD-2, SEI-2, AUL-2, STD-1 \\
\midrule
\textbf{FRQ-4:} The system shall permanently store and retain digital receipt submissions. &
SaL-3, PRE-1, FLT-2, CAP-2, LNG-1, PRI-1, PRI-2, AUD-2, SEI-1, SEI-2, SEI-3, AUL-2, RDL-1, STD-1 \\
\midrule
\textbf{FRQ-5:} The system shall maintain an audit trail that records who submitted, reviewed, approved, or denied each expense claim. &
FLT-2, CAP-2, LNG-1, ACS-2, PRI-2, AUD-1, AUD-2, IMM-1, SEI-2, SEI-3, AUL-1, AUL-2, RDL-1, STD-1, STD-3 \\
\midrule
\textbf{FRQ-6:} The system shall enable access to club expense submissions to the submitters, and other club members with a role greater or equal to that of the submitter, or MES administrators and approvers. &
APP-3, ABL-1, ABL-5, SaL-1, SaL-2, CAP-1, EXT-2, ACS-1, ACS-2, INT-1, PRI-1, PRI-2, AUD-1, SEI-2, AUL-1, AUL-2, STD-1, STD-2 \\
\bottomrule
\end{longtable}

\subsection*{Legend: Non-Functional Requirement Categories}
\begin{itemize}
    \item \textbf{APP / STY:} Look and Feel Requirements (Appearance, Style)
    \item \textbf{EUL / UaP / ABL:} Usability and Humanity Requirements (Ease of Use, Understandability, Accessibility)
    \item \textbf{SaL / PRE / FLT / CAP / EXT / LNG:} Performance Requirements (Speed, Precision, Fault-Tolerance, Capacity, Extensibility, Longevity)
    \item \textbf{ACS / INT / PRI / AUD / IMM:} Security Requirements (Access, Integrity, Privacy, Audit, Immunity)
    \item \textbf{CUL:} Cultural Requirements
    \item \textbf{SEI / AUL / RDL / STD:} Compliance Requirements (Storage/Encryption/Infrastructure, Access/Audit/Logging, Retention/Disposal, Standards)
\end{itemize}

\noindent This matrix ensures that for every function the system performs, the corresponding quality attributes are verified through specific test cases.

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{document}